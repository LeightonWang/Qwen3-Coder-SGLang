# ä½¿ç”¨ SGLang éƒ¨ç½² Qwen3-0.6B
[English](README.md)

è¿™ä¸ªä»“åº“æ˜¯ä¸€ä¸ªä½œä¸šçš„å®ç°ã€‚ç›®æ ‡æ˜¯ä½¿ç”¨ SGLang éƒ¨ç½² Qwen3-0.6B æ¨¡å‹ï¼Œç„¶ååœ¨ HumanEval æ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†ï¼Œæœ€åå¯¹ç»“æœè¿›è¡Œè¯„ä¼°ã€‚

## å‰ç½®æ¡ä»¶
å…¶ä»–ç‰ˆæœ¬çš„è½¯ä»¶å’ŒåŒ…ä¹Ÿå¯èƒ½å¯ç”¨ã€‚è¿™é‡Œåªæ˜¯æˆ‘åœ¨è‡ªå·±ç¯å¢ƒä¸­ä½¿ç”¨çš„ç‰ˆæœ¬ã€‚
- OS: Ubuntu 22.04
- Docker 28.3.3
- CUDA 12.4.1
- Nvidia Container Toolkit 1.17.8-1
- Python 3.10
  - transformers 4.55.4
  - torch 2.8.0
  - requests 2.32.5

## 1. ä½¿ç”¨ SGLang éƒ¨ç½²æ¨¡å‹

åªéœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
python3 client.py [OPTIONS]
```

ç„¶åä¼šå¯åŠ¨ä¸€ä¸ªåŒ…å« SGLang æœåŠ¡çš„ Docker å®¹å™¨ã€‚å¯ç”¨å‚æ•°å¦‚ä¸‹ï¼š
| å‚æ•°              | è¯´æ˜                                                                 | é»˜è®¤å€¼                                      |
|---------------------|----------------------------------------------------------------------|---------------------------------------------|
| `--model-name`      |  æ¨¡å‹åç§°                                                          | Qwen3-0.6B                                  |
| `--port`            | æœåŠ¡ç«¯å£å·                                                         | 30000                                       |
| `--tp`              | GPU æ•°                | 1                                           |
| `--local-save-path` | æœ¬åœ°æ¨¡å‹è·¯å¾„                                              | /NV/models_hf/Qwen/Qwen3-0.6B               |
| `--container-name`  | Docker å®¹å™¨å                                                | qwen-test                                   |
| `--image`           | Docker é•œåƒå                                                    | sglang/sglang:latest                        |

æ‰€æœ‰é»˜è®¤å€¼å‡ä¸ºæˆ‘è‡ªå·±ç¯å¢ƒä¸­çš„é…ç½®ï¼Œæ–¹ä¾¿èµ·è§ã€‚ä½ å¯èƒ½éœ€è¦æ ¹æ®è‡ªå·±çš„ç¯å¢ƒè¿›è¡Œä¿®æ”¹ã€‚


### ä¸å·²éƒ¨ç½²çš„æ¨¡å‹ç®€å•å¯¹è¯

ç°åœ¨ Qwen3-0.6B å·²ç»éƒ¨ç½²åœ¨ `http://127.0.0.1:30000`. å¦‚æœæƒ³è¦è¿›è¡Œç®€å•çš„å¯¹è¯ï¼Œåªéœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
curl --location --request POST 'http://127.0.0.1:30000/v1/chat/completions' --header 'Content-Type: application/json' --data-raw '{
    "model": "Qwen/Qwen3-0.6B",
    "messages": [
      {"role": "user", "content": "Damn, I finally deployed you successfully"}
    ],
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

è¿”å›ç»“æœåº”ç±»ä¼¼å¦‚ä¸‹ï¼š
```json
{"id":"12730b4f37924164a9d051d8d8c65d43","object":"chat.completion","created":1756301670,"model":"Qwen/Qwen3-0.6B","choices":[{"index":0,"message":{"role":"assistant","content":"Great job! ğŸ‰ I'm really glad you did it. If you have any questions or need further assistance, feel free to ask! ğŸ˜Š","reasoning_content":"Okay, the user said \"Damn, I finally deployed you successfully.\" Let me think about how to respond. First, I need to acknowledge their success. Maybe start with something like, \"Great job!\" to show excitement. Then, add something helpful, like confirming that the deployment was successful and offer further assistance. Keep the tone positive and open for questions. Avoid any negative language, so just focus on celebration and support. Make sure the response is friendly and engaging.\n","tool_calls":null},"logprobs":null,"finish_reason":"stop","matched_stop":151645}],"usage":{"prompt_tokens":15,"total_tokens":145,"completion_tokens":130,"prompt_tokens_details":null}}
```

## 2. æ¨ç†
åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘å°†å¼€å‘ä¸€ä¸ªè„šæœ¬åœ¨ HumanEval æ•°æ®é›†ä¸Šæ‰§è¡Œæ¨ç†ã€‚è¯¥è„šæœ¬åº”ä¸å·²éƒ¨ç½²çš„æ¨¡å‹äº¤äº’ï¼Œä¸ºæä¾›çš„æ ·æœ¬ç”Ÿæˆé¢„æµ‹ã€‚

### 2.1 æ•°æ®é›†
åœ¨ç›®å½•ä¸­æœ‰ä¸¤ä¸ªæ•°æ®é›† `datasets/`:
```text
.
â”œâ”€â”€ datasets
    â”œâ”€â”€ HumanEval_4.jsonl
    â””â”€â”€ HumanEval.jsonl
```
`HumanEval.jsonl` æ˜¯å®Œæ•´çš„ HumanEval æ•°æ®é›†ã€‚ `HumanEval_4.jsonl` æ˜¯å®Œæ•´æ•°æ®é›†çš„ä¸€ä¸ªå°å­é›†ï¼Œä»…åŒ…å« 4 ä¸ªæ ·æœ¬ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•å’Œè°ƒè¯•ã€‚

### 2.2 æ¨ç†è„šæœ¬
æ¨ç†è„šæœ¬æ˜¯ `inference/inference_he.py`. è¿è¡Œæ¨ç†ï¼š
```bash
cd inference
python3 inference_he.py [OPTIONS]
```
å¯ç”¨é€‰é¡¹ï¼š

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `-o`, `--output_file` | è¾“å‡ºæ–‡ä»¶åã€‚å­˜å‚¨åœ¨ `outputs/` | `humaneval_results.jsonl`. |
| `--think` | æ˜¯å¦å¯ç”¨ think æ¨¡å¼ã€‚ | `False` |
| `--debug` | è°ƒè¯•æ¨¡å¼ã€‚å¦‚æœå¯ç”¨ï¼Œå°†ä½¿ç”¨ 4 ä¸ªæ ·æœ¬å­é›†ã€‚ | `False` |

ä¾‹å¦‚ï¼Œè¦åœ¨å°å­é›†ä¸Šè¿è¡Œæ¨ç†å¹¶å¯ç”¨ think æ¨¡å¼ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åˆ° `he_results_debug_think_results.jsonl`:
```bash
python3 inference_he.py --think --debug -o he_results_debug_think_results.jsonl
```

## 3. è¯„ä¼°
è¿™ä¸€éƒ¨åˆ†ï¼Œéœ€è¦æ­å»ºä¸€ä¸ªæ²™ç®±ç¯å¢ƒæ¥è¯„ä¼°å‰ä¸€æ­¥å¾—åˆ°çš„ HumanEval ç»“æœçš„é€šè¿‡ç‡ã€‚

### 3.1 æ„å»º Docker é•œåƒ
æˆ‘åœ¨æ ¹ç›®å½•å‡†å¤‡äº†ä¸€ä¸ª Dockerfileï¼Œç”¨äºæ„å»ºæœ€å°åŒ–çš„è¯„ä¼°é•œåƒã€‚åªéœ€è¿è¡Œï¼š
```bash
docker build -t humaneval_eval:latest .
```
é•œåƒå°†è¢«æ„å»ºå®Œæˆã€‚

### 3.2 æ‰§è¡Œè¯„ä¼°
é•œåƒæ„å»ºå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œè¯„ä¼°ã€‚æä¾›äº†ä¸¤ç§æ–¹æ³•ï¼šå‘½ä»¤è¡Œå’Œè„šæœ¬ï¼Œå…¶ä¸­è„šæœ¬ä¼šè‡ªåŠ¨åŒ–æ•´ä¸ªæµç¨‹ã€‚

#### 3.2.1 åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œè¯„ä¼°
æˆ‘ä»¬å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œè¯„ä¼°ã€‚å¯åŠ¨å®¹å™¨ï¼š
```bash
docker run -it --rm \
    -v $(pwd)/results:/app/results \
    -v $(pwd)/datasets:/app/datasets \
    humaneval_eval:latest
```

ç„¶ååœ¨å®¹å™¨ä¸­è¿è¡Œè¯„ä¼°è„šæœ¬ï¼š
```bash
cd evaluate
python3 evaluate.py [OPTIONS]
```
å¯ç”¨é€‰é¡¹ï¼š
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `-f`, `--file` | The output file to be evaluated. | `../outputs/he_results_no_think.jsonl`. |

è¯„ä¼°ç»“æœå°†ä¼šæ‰“å°åœ¨ç»ˆç«¯ä¸­ã€‚

#### 3.2.2 é€šè¿‡è„šæœ¬æ‰§è¡Œè¯„ä¼°
æˆ‘è¿˜å‡†å¤‡äº†ä¸€ä¸ªè„šæœ¬ `auto-evaluate.py` åœ¨æ ¹ç›®å½•ä¸­è‡ªåŠ¨åŒ–æ‰§è¡Œè¯„ä¼°è¿‡ç¨‹ã€‚åªéœ€è¿è¡Œï¼š
```bash
python3 auto-evaluate.py [OPTIONS]
```
å¯ç”¨é€‰é¡¹ï¼š
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `-f`, `--file` | å¾…è¯„ä¼°æ–‡ä»¶ï¼Œåœ¨ `outputs/` ç›®å½•ä¸‹ | `humaneval_results.jsonl`. |  
| `-o`, `--output` | è¯„ä¼°æŠ¥å‘Šï¼Œå­˜å‚¨åœ¨`results/` ç›®å½•ä¸‹ | eval_report.jsonl |

æ¥ä¸‹æ¥è„šæœ¬ä¼šè‡ªåŠ¨å¯åŠ¨ä¸€ä¸ªå®¹å™¨å¹¶æ‰§è¡Œè¯„ä¼°ã€‚è¿‡ç¨‹ä¸å‘½ä»¤è¡Œæ–¹æ³•ä¸€è‡´ã€‚

### 3.3 è¯„ä¼°ç»“æœ
é€šè¿‡ç‡ç»“æœå¦‚ä¸‹ï¼š
| Think Mode | System Prompt | Output File | Pass Rate |
|------------|---------------|-------------|-----------|
| No         | é»˜è®¤å€¼       | he_outputs_pure_code.jsonl | 89.02% (146/164) |

# è¿è¡Œç¤ºä¾‹
å¯åŠ¨ SGLang åç«¯
```bash
python3 client.py
```
åœ¨ HumanEval æ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†ï¼š
```bash
# (in a new terminal)
cd inference
python3 inference_he.py -o he_outputs_pure_code.jsonl
```
æ¨ç†ç»“æœå·²å­˜å‚¨åœ¨ `outputs/he_outputs_pure_code.jsonl`.

ä½¿ç”¨ pass@1 æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼š
```bash
cd ..
python3 auto-evaluate.py -f he_outputs_pure_code.jsonl -o eval_report.jsonl
```
è¯„ä¼°æŠ¥å‘Šå­˜å‚¨åœ¨ `results/eval_report.jsonl`ã€‚

## æ•…éšœæ’æŸ¥
è¿™ä¸€éƒ¨åˆ†ä¸»è¦æ˜¯ä¸ºè‡ªå·±è®°å½•åœ¨éƒ¨ç½²å’Œå¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„ä¸€äº›é—®é¢˜ã€‚

1. **æ— æ³•è®¿é—® HuggingFace**ï¼šæˆ‘è¿™é‡Œæ— æ³•è®¿é—®å®˜æ–¹æºã€‚æ·»åŠ  `HF_ENDPOINT` åˆ°ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨é•œåƒç«™ã€‚
    ```bash
    export HF_ENDPOINT=https://hf-mirror.com
    ```
2. **æ— æ³•è·å– Nvidia Container Toolkit çš„ GPG å¯†é’¥**ï¼šæˆ‘è¿™é‡Œæ— æ³•è®¿é—®å®˜æ–¹æºã€‚ä½¿ç”¨[ç§‘å¤§æº](https://mirrors.ustc.edu.cn/help/libnvidia-container.html) ã€‚